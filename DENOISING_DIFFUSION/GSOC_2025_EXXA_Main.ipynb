{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1g3Q0atGa_Leyt5udIFXs5a7HsCL-gGmv/view?usp=sharing > /dev/null\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1EYZzQg-PNXWN1vpD0BLSbfG6jyP0nTVr/view?usp=sharing > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T152UOG6t5M8",
        "outputId": "24d66e41-f5c9-4fec-b8ba-7cb40223d60b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1g3Q0atGa_Leyt5udIFXs5a7HsCL-gGmv\n",
            "From (redirected): https://drive.google.com/uc?id=1g3Q0atGa_Leyt5udIFXs5a7HsCL-gGmv&confirm=t&uuid=42d54ce9-85c5-4b4d-a1f1-36172178af6d\n",
            "To: /content/clean.npy\n",
            "100% 1.40G/1.40G [00:07<00:00, 181MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1EYZzQg-PNXWN1vpD0BLSbfG6jyP0nTVr\n",
            "From (redirected): https://drive.google.com/uc?id=1EYZzQg-PNXWN1vpD0BLSbfG6jyP0nTVr&confirm=t&uuid=f8b44264-e194-4ef8-808d-0c14e5938715\n",
            "To: /content/dirty.npy\n",
            "100% 2.81G/2.81G [00:25<00:00, 110MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "\n",
        "!git clone https://github.com/jorge-pessoa/pytorch-msssim > /dev/null\n",
        "%cd pytorch-msssim\n",
        "!python setup.py install > /dev/null\n",
        "import pytorch_msssim\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "irW935T5uRFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = np.load(\"dirty.npy\")\n",
        "targets = np.load(\"clean.npy\")\n",
        "#m = pytorch_msssim.MSSSIM()\n",
        "\n",
        "# Split keeping correspondences\n",
        "train_data, val_data, train_targets, val_targets = train_test_split(\n",
        "    data, targets,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "del data, targets\n",
        "\n",
        "noisy = val_data\n",
        "clean = val_targets"
      ],
      "metadata": {
        "id": "6m1x0d5-vHpC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyperparameters\n",
        "\n",
        "import yaml\n",
        "\n",
        "config = {\n",
        "    \"data\": {\n",
        "        \"dataset\": \"Astro\",\n",
        "        \"image_size\": 64,\n",
        "        \"channels\": 1,\n",
        "        \"num_workers\": 1,\n",
        "        \"data_dir\": \"/scratch/ozan/\",\n",
        "        \"conditional\": True\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"in_channels\": 1,\n",
        "        \"out_ch\": 1,\n",
        "        \"ch\": 128,\n",
        "        \"ch_mult\": [1, 1, 2, 2, 4, 4],\n",
        "        \"num_res_blocks\": 2,\n",
        "        \"attn_resolutions\": [16],\n",
        "        \"dropout\": 0.0,\n",
        "        \"ema_rate\": 0.999,\n",
        "        \"ema\": True,\n",
        "        \"resamp_with_conv\": True\n",
        "    },\n",
        "    \"diffusion\": {\n",
        "        \"beta_schedule\": \"linear\",\n",
        "        \"beta_start\": 0.0001,\n",
        "        \"beta_end\": 0.02,\n",
        "        \"num_diffusion_timesteps\": 1000\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"patch_n\": 4,\n",
        "        \"batch_size\": 4,\n",
        "        \"n_epochs\": 641,\n",
        "        \"n_iters\": 2000000,\n",
        "        \"snapshot_freq\": 10000,\n",
        "        \"validation_freq\": 10000\n",
        "    },\n",
        "    \"sampling\": {\n",
        "        \"batch_size\": 4,\n",
        "        \"last_only\": True\n",
        "    },\n",
        "    \"optim\": {\n",
        "        \"weight_decay\": 0.0,\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"lr\": 0.00002,\n",
        "        \"amsgrad\": False,\n",
        "        \"eps\": 1e-8\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"config.yml\", \"w\") as file:\n",
        "    yaml.dump(config, file, default_flow_style=False)\n",
        "\n",
        "print(\"config.yml created.\")\n",
        "\n",
        "class DotDict(dict):\n",
        "    \"\"\"Allows dot notation access to dictionary attributes.\"\"\"\n",
        "    def __getattr__(self, attr):\n",
        "        value = self.get(attr)\n",
        "        if isinstance(value, dict):\n",
        "            return DotDict(value)\n",
        "        return value\n",
        "\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "config = DotDict(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "BXSPZAIGyRbO",
        "outputId": "005946dd-5cf6-4142-e0dc-bb0aaec5b5b6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.yml created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Class\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "import PIL\n",
        "import re\n",
        "import random\n",
        "\n",
        "class AstroDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_array, gt_array, patch_size, n, transforms, parse_patches=True):\n",
        "        super().__init__()\n",
        "        assert input_array.shape == gt_array.shape\n",
        "        self.input_array = input_array  # Shape: (N, H, W)\n",
        "        self.gt_array = gt_array\n",
        "        self.patch_size = patch_size\n",
        "        self.transforms = transforms\n",
        "        self.n = n\n",
        "        self.parse_patches = parse_patches\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(h, w, output_size, n):\n",
        "        th, tw = output_size\n",
        "        i_list = [random.randint(0, h - th) for _ in range(n)]\n",
        "        j_list = [random.randint(0, w - tw) for _ in range(n)]\n",
        "        return i_list, j_list, th, tw\n",
        "\n",
        "    @staticmethod\n",
        "    def n_random_crops(img, i_list, j_list, h, w):\n",
        "        crops = [img[i:i+h, j:j+w] for i, j in zip(i_list, j_list)]\n",
        "        return crops\n",
        "\n",
        "    def get_images(self, index):\n",
        "        input_img = self.input_array[index]  # Shape: (H, W)\n",
        "        gt_img = self.gt_array[index]        # Shape: (H, W)\n",
        "        img_id = f\"{index:05d}\"\n",
        "\n",
        "        if self.parse_patches:\n",
        "            h, w = input_img.shape\n",
        "            i, j, ph, pw = self.get_params(h, w, (self.patch_size, self.patch_size), self.n)\n",
        "            input_crops = self.n_random_crops(input_img, i, j, ph, pw)\n",
        "            gt_crops = self.n_random_crops(gt_img, i, j, ph, pw)\n",
        "\n",
        "            outputs = []\n",
        "            for inp, gt in zip(input_crops, gt_crops):\n",
        "                inp_tensor = self.transforms(inp[np.newaxis, ...])  # Add channel dim\n",
        "                gt_tensor = self.transforms(gt[np.newaxis, ...])\n",
        "                outputs.append(torch.cat([inp_tensor, gt_tensor], dim=0))\n",
        "            return torch.stack(outputs, dim=0), img_id\n",
        "        else:\n",
        "            # Resize to multiples of 16\n",
        "            h, w = input_img.shape\n",
        "            if h > w and h > 1024:\n",
        "                w = int(np.ceil(w * 1024 / h))\n",
        "                h = 1024\n",
        "            elif w >= h and w > 1024:\n",
        "                h = int(np.ceil(h * 1024 / w))\n",
        "                w = 1024\n",
        "            h = int(16 * np.ceil(h / 16.0))\n",
        "            w = int(16 * np.ceil(w / 16.0))\n",
        "\n",
        "            input_resized = torch.nn.functional.interpolate(\n",
        "                torch.from_numpy(input_img[np.newaxis, np.newaxis, ...]).float(), size=(h, w), mode='bilinear', align_corners=False\n",
        "            ).squeeze(0)\n",
        "\n",
        "            gt_resized = torch.nn.functional.interpolate(\n",
        "                torch.from_numpy(gt_img[np.newaxis, np.newaxis, ...]).float(), size=(h, w), mode='bilinear', align_corners=False\n",
        "            ).squeeze(0)\n",
        "\n",
        "            return torch.cat([input_resized, gt_resized], dim=0), img_id\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.get_images(index)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.input_array.shape[0]\n",
        "\n",
        "\n",
        "class Astro:\n",
        "    def __init__(self, config, train_input_array, train_gt_array, val_input_array, val_gt_array):\n",
        "        self.config = config\n",
        "        self.train_input_array = train_input_array\n",
        "        self.train_gt_array = train_gt_array\n",
        "        self.val_input_array = val_input_array\n",
        "        self.val_gt_array = val_gt_array\n",
        "        self.transforms = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Lambda(lambda x: torch.from_numpy(x).float())\n",
        "        ])\n",
        "\n",
        "    def get_loaders(self, parse_patches=True):\n",
        "        print(\"=> loading grayscale numpy Astro dataset...\")\n",
        "\n",
        "        train_dataset = AstroDataset(\n",
        "            input_array=self.train_input_array,\n",
        "            gt_array=self.train_gt_array,\n",
        "            n=self.config.training.patch_n,\n",
        "            patch_size=self.config.data.image_size,\n",
        "            transforms=self.transforms,\n",
        "            parse_patches=parse_patches\n",
        "        )\n",
        "\n",
        "        val_dataset = AstroDataset(\n",
        "            input_array=self.val_input_array,\n",
        "            gt_array=self.val_gt_array,\n",
        "            n=self.config.training.patch_n,\n",
        "            patch_size=self.config.data.image_size,\n",
        "            transforms=self.transforms,\n",
        "            parse_patches=parse_patches\n",
        "        )\n",
        "\n",
        "        if not parse_patches:\n",
        "            self.config.training.batch_size = 1\n",
        "            self.config.sampling.batch_size = 1\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.config.training.batch_size,\n",
        "                                                   shuffle=True, num_workers=self.config.data.num_workers,\n",
        "                                                   pin_memory=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=self.config.sampling.batch_size,\n",
        "                                                 shuffle=False, num_workers=self.config.data.num_workers,\n",
        "                                                 pin_memory=True)\n",
        "\n",
        "        return train_loader, val_loader\n"
      ],
      "metadata": {
        "id": "h40xfkvryBj-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "astro_dataset = Astro(config, train_data, train_targets, val_data, val_targets)\n",
        "train_loader, val_loader = astro_dataset.get_loaders()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrt3CqHKxWro",
        "outputId": "ab5da725-396d-4c34-bd24-9ccd8abb3c36"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> loading grayscale numpy Astro dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utils\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import shutil\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torchvision.utils as tvu\n",
        "from torchvision.transforms.functional import crop\n",
        "\n",
        "\n",
        "def save_image(img, file_directory):\n",
        "    if not os.path.exists(os.path.dirname(file_directory)):\n",
        "        os.makedirs(os.path.dirname(file_directory))\n",
        "    tvu.save_image(img, file_directory)\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    if not os.path.exists(os.path.dirname(filename)):\n",
        "        os.makedirs(os.path.dirname(filename))\n",
        "    torch.save(state, filename + '.pth.tar')\n",
        "\n",
        "\n",
        "def load_checkpoint(path, device):\n",
        "    if device is None:\n",
        "        return torch.load(path)\n",
        "    else:\n",
        "        return torch.load(path, map_location=device)\n",
        "\n",
        "\n",
        "# This script is adapted from the following repository: https://github.com/JingyunLiang/SwinIR\n",
        "\n",
        "\n",
        "def get_optimizer(config, parameters):\n",
        "    if config.optim.optimizer == 'Adam':\n",
        "        return optim.Adam(parameters, lr=config.optim.lr, weight_decay=config.optim.weight_decay,\n",
        "                          betas=(0.9, 0.999), amsgrad=config.optim.amsgrad, eps=config.optim.eps)\n",
        "    elif config.optim.optimizer == 'RMSProp':\n",
        "        return optim.RMSprop(parameters, lr=config.optim.lr, weight_decay=config.optim.weight_decay)\n",
        "    elif config.optim.optimizer == 'SGD':\n",
        "        return optim.SGD(parameters, lr=config.optim.lr, momentum=0.9)\n",
        "    else:\n",
        "        raise NotImplementedError('Optimizer {} not understood.'.format(config.optim.optimizer))\n",
        "\n",
        "\n",
        "\n",
        "# This script is adapted from the following repository: https://github.com/ermongroup/ddim\n",
        "\n",
        "def compute_alpha(beta, t):\n",
        "    beta = torch.cat([torch.zeros(1).to(beta.device), beta], dim=0)\n",
        "    a = (1 - beta).cumprod(dim=0).index_select(0, t + 1).view(-1, 1, 1, 1)\n",
        "    return a\n",
        "\n",
        "\n",
        "def data_transform(X):\n",
        "    return 2 * X - 1.0\n",
        "\n",
        "\n",
        "def inverse_data_transform(X):\n",
        "    return torch.clamp((X + 1.0) / 2.0, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def generalized_steps(x, x_cond, seq, model, b, eta=0.):\n",
        "    with torch.no_grad():\n",
        "        n = x.size(0)\n",
        "        seq_next = [-1] + list(seq[:-1])\n",
        "        x0_preds = []\n",
        "        xs = [x]\n",
        "        for i, j in zip(reversed(seq), reversed(seq_next)):\n",
        "            t = (torch.ones(n) * i).to(x.device)\n",
        "            next_t = (torch.ones(n) * j).to(x.device)\n",
        "            at = compute_alpha(b, t.long())\n",
        "            at_next = compute_alpha(b, next_t.long())\n",
        "            xt = xs[-1].to('cuda')\n",
        "\n",
        "            et = model(torch.cat([x_cond, xt], dim=1), t)\n",
        "            x0_t = (xt - et * (1 - at).sqrt()) / at.sqrt()\n",
        "            x0_preds.append(x0_t.to('cpu'))\n",
        "\n",
        "            c1 = eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt()\n",
        "            c2 = ((1 - at_next) - c1 ** 2).sqrt()\n",
        "            xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x) + c2 * et\n",
        "            xs.append(xt_next.to('cpu'))\n",
        "    return xs, x0_preds\n",
        "\n",
        "\n",
        "def generalized_steps_overlapping(x, x_cond, seq, model, b, eta=0., corners=None, p_size=None, manual_batching=True):\n",
        "    with torch.no_grad():\n",
        "        n = x.size(0)\n",
        "        seq_next = [-1] + list(seq[:-1])\n",
        "        x0_preds = []\n",
        "        xs = [x]\n",
        "\n",
        "        x_grid_mask = torch.zeros_like(x_cond, device=x.device)\n",
        "        for (hi, wi) in corners:\n",
        "            x_grid_mask[:, :, hi:hi + p_size, wi:wi + p_size] += 1\n",
        "\n",
        "        for i, j in zip(reversed(seq), reversed(seq_next)):\n",
        "            t = (torch.ones(n) * i).to(x.device)\n",
        "            next_t = (torch.ones(n) * j).to(x.device)\n",
        "            at = compute_alpha(b, t.long())\n",
        "            at_next = compute_alpha(b, next_t.long())\n",
        "            xt = xs[-1].to('cuda')\n",
        "            et_output = torch.zeros_like(x_cond, device=x.device)\n",
        "\n",
        "            if manual_batching:\n",
        "                manual_batching_size = 64\n",
        "                xt_patch = torch.cat([crop(xt, hi, wi, p_size, p_size) for (hi, wi) in corners], dim=0)\n",
        "                x_cond_patch = torch.cat([data_transform(crop(x_cond, hi, wi, p_size, p_size)) for (hi, wi) in corners], dim=0)\n",
        "                for i in range(0, len(corners), manual_batching_size):\n",
        "                    outputs = model(torch.cat([x_cond_patch[i:i+manual_batching_size],\n",
        "                                               xt_patch[i:i+manual_batching_size]], dim=1), t)\n",
        "                    for idx, (hi, wi) in enumerate(corners[i:i+manual_batching_size]):\n",
        "                        et_output[0, :, hi:hi + p_size, wi:wi + p_size] += outputs[idx]\n",
        "            else:\n",
        "                for (hi, wi) in corners:\n",
        "                    xt_patch = crop(xt, hi, wi, p_size, p_size)\n",
        "                    x_cond_patch = crop(x_cond, hi, wi, p_size, p_size)\n",
        "                    x_cond_patch = data_transform(x_cond_patch)\n",
        "                    et_output[:, :, hi:hi + p_size, wi:wi + p_size] += model(torch.cat([x_cond_patch, xt_patch], dim=1), t)\n",
        "\n",
        "            et = torch.div(et_output, x_grid_mask)\n",
        "            x0_t = (xt - et * (1 - at).sqrt()) / at.sqrt()\n",
        "            x0_preds.append(x0_t.to('cpu'))\n",
        "\n",
        "            c1 = eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt()\n",
        "            c2 = ((1 - at_next) - c1 ** 2).sqrt()\n",
        "            xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x) + c2 * et\n",
        "            xs.append(xt_next.to('cpu'))\n",
        "    return xs, x0_preds"
      ],
      "metadata": {
        "cellView": "form",
        "id": "phwcld9-xgYQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# This script is from the following repositories\n",
        "# https://github.com/ermongroup/ddim\n",
        "# https://github.com/bahjat-kawar/ddrm\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "    \"\"\"\n",
        "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
        "    From Fairseq.\n",
        "    Build sinusoidal embeddings.\n",
        "    This matches the implementation in tensor2tensor, but differs slightly\n",
        "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n",
        "    return emb\n",
        "\n",
        "\n",
        "def nonlinearity(x):\n",
        "    # swish\n",
        "    return x*torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def Normalize(in_channels):\n",
        "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(\n",
        "            x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=2,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0, 1, 0, 1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels,\n",
        "                                     out_channels,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)\n",
        "        self.temb_proj = torch.nn.Linear(temb_channels,\n",
        "                                         out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels,\n",
        "                                     out_channels,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n",
        "                                                     out_channels,\n",
        "                                                     kernel_size=3,\n",
        "                                                     stride=1,\n",
        "                                                     padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n",
        "                                                    out_channels,\n",
        "                                                    kernel_size=1,\n",
        "                                                    stride=1,\n",
        "                                                    padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        h = h + self.temb_proj(nonlinearity(temb))[:, :, None, None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b, c, h, w = q.shape\n",
        "        q = q.reshape(b, c, h*w)\n",
        "        q = q.permute(0, 2, 1)   # b,hw,c\n",
        "        k = k.reshape(b, c, h*w)  # b,c,hw\n",
        "        w_ = torch.bmm(q, k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b, c, h*w)\n",
        "        w_ = w_.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n",
        "        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = torch.bmm(v, w_)\n",
        "        h_ = h_.reshape(b, c, h, w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "class DiffusionUNet(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        ch, out_ch, ch_mult = config.model.ch, config.model.out_ch, tuple(config.model.ch_mult)\n",
        "        num_res_blocks = config.model.num_res_blocks\n",
        "        attn_resolutions = config.model.attn_resolutions\n",
        "        dropout = config.model.dropout\n",
        "        in_channels = config.model.in_channels * 2 if config.data.conditional else config.model.in_channels\n",
        "        resolution = config.data.image_size\n",
        "        resamp_with_conv = config.model.resamp_with_conv\n",
        "\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # timestep embedding\n",
        "        self.temb = nn.Module()\n",
        "        self.temb.dense = nn.ModuleList([\n",
        "            torch.nn.Linear(self.ch,\n",
        "                            self.temb_ch),\n",
        "            torch.nn.Linear(self.temb_ch,\n",
        "                            self.temb_ch),\n",
        "        ])\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        block_in = None\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(AttnBlock(block_in))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = AttnBlock(block_in)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level]\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(AttnBlock(block_in))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up)  # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in,\n",
        "                                        out_ch,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        assert x.shape[2] == x.shape[3] == self.resolution\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = get_timestep_embedding(t, self.ch)\n",
        "        temb = self.temb.dense[0](temb)\n",
        "        temb = nonlinearity(temb)\n",
        "        temb = self.temb.dense[1](temb)\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](\n",
        "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E2h5YUi51klO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Diffusion Algorithm\n",
        "\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm as tm\n",
        "import torch.utils.data as data\n",
        "import torch.backends.cudnn as cudnn\n",
        "#import utils\n",
        "#from models.unet import DiffusionUNet\n",
        "\n",
        "\n",
        "# This script is adapted from the following repositories\n",
        "# https://github.com/ermongroup/ddim\n",
        "# https://github.com/bahjat-kawar/ddrm\n",
        "\n",
        "\n",
        "def data_transform(X):\n",
        "    return 2 * X - 1.0\n",
        "\n",
        "\n",
        "def inverse_data_transform(X):\n",
        "    return torch.clamp((X + 1.0) / 2.0, 0.0, 1.0)\n",
        "\n",
        "\n",
        "class EMAHelper(object):\n",
        "    def __init__(self, mu=0.9999):\n",
        "        self.mu = mu\n",
        "        self.shadow = {}\n",
        "\n",
        "    def register(self, module):\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self, module):\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name].data = (1. - self.mu) * param.data + self.mu * self.shadow[name].data\n",
        "\n",
        "    def ema(self, module):\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data.copy_(self.shadow[name].data)\n",
        "\n",
        "    def ema_copy(self, module):\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            inner_module = module.module\n",
        "            module_copy = type(inner_module)(inner_module.config).to(inner_module.config.device)\n",
        "            module_copy.load_state_dict(inner_module.state_dict())\n",
        "            module_copy = nn.DataParallel(module_copy)\n",
        "        else:\n",
        "            module_copy = type(module)(module.config).to(module.config.device)\n",
        "            module_copy.load_state_dict(module.state_dict())\n",
        "        self.ema(module_copy)\n",
        "        return module_copy\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.shadow\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.shadow = state_dict\n",
        "\n",
        "\n",
        "def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n",
        "    def sigmoid(x):\n",
        "        return 1 / (np.exp(-x) + 1)\n",
        "\n",
        "    if beta_schedule == \"quad\":\n",
        "        betas = (np.linspace(beta_start ** 0.5, beta_end ** 0.5, num_diffusion_timesteps, dtype=np.float64) ** 2)\n",
        "    elif beta_schedule == \"linear\":\n",
        "        betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n",
        "    elif beta_schedule == \"const\":\n",
        "        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n",
        "    elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
        "        betas = 1.0 / np.linspace(num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64)\n",
        "    elif beta_schedule == \"sigmoid\":\n",
        "        betas = np.linspace(-6, 6, num_diffusion_timesteps)\n",
        "        betas = sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
        "    else:\n",
        "        raise NotImplementedError(beta_schedule)\n",
        "    assert betas.shape == (num_diffusion_timesteps,)\n",
        "    return betas\n",
        "\n",
        "\n",
        "def noise_estimation_loss(model, x0, t, e, b):\n",
        "    a = (1-b).cumprod(dim=0).index_select(0, t).view(-1, 1, 1, 1)\n",
        "    x = x0[:, 1:, :, :] * a.sqrt() + e * (1.0 - a).sqrt()\n",
        "    #output = model(x, t.float())\n",
        "    output = model(torch.cat([x0[:, :1, :, :], x], dim=1), t.float())\n",
        "    return (e - output).square().sum(dim=(1, 2, 3)).mean(dim=0)\n",
        "\n",
        "\n",
        "class DenoisingDiffusion(object):\n",
        "    def __init__(self, args, config):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "\n",
        "        self.model = DiffusionUNet(config)\n",
        "        self.model.to(self.device)\n",
        "        self.model = torch.nn.DataParallel(self.model)\n",
        "\n",
        "        self.ema_helper = EMAHelper()\n",
        "        self.ema_helper.register(self.model)\n",
        "\n",
        "        self.optimizer = get_optimizer(self.config, self.model.parameters())\n",
        "        self.start_epoch, self.step = 0, 0\n",
        "\n",
        "        betas = get_beta_schedule(\n",
        "            beta_schedule=config.diffusion.beta_schedule,\n",
        "            beta_start=config.diffusion.beta_start,\n",
        "            beta_end=config.diffusion.beta_end,\n",
        "            num_diffusion_timesteps=config.diffusion.num_diffusion_timesteps,\n",
        "        )\n",
        "\n",
        "        betas = self.betas = torch.from_numpy(betas).float().to(self.device)\n",
        "        self.num_timesteps = betas.shape[0]\n",
        "\n",
        "    def load_ddm_ckpt(self, load_path, ema=False):\n",
        "        checkpoint = load_checkpoint(load_path, None)\n",
        "        self.start_epoch = checkpoint['epoch']\n",
        "        self.step = checkpoint['step']\n",
        "        self.model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.ema_helper.load_state_dict(checkpoint['ema_helper'])\n",
        "        if ema:\n",
        "            self.ema_helper.ema(self.model)\n",
        "        print(\"=> loaded checkpoint '{}' (epoch {}, step {})\".format(load_path, checkpoint['epoch'], self.step))\n",
        "\n",
        "    def train(self, DATASET):\n",
        "        cudnn.benchmark = True\n",
        "        train_loader, val_loader = DATASET.get_loaders()\n",
        "\n",
        "        if os.path.isfile(self.args.resume):\n",
        "            self.load_ddm_ckpt(self.args.resume)\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.config.training.n_epochs):\n",
        "            print('epoch: ', epoch)\n",
        "            data_start = time.time()\n",
        "            data_time = 0\n",
        "            for i, (x, y) in enumerate(train_loader):\n",
        "                x = x.flatten(start_dim=0, end_dim=1) if x.ndim == 5 else x\n",
        "                n = x.size(0)\n",
        "                data_time += time.time() - data_start\n",
        "                self.model.train()\n",
        "                self.step += 1\n",
        "\n",
        "                x = x.to(self.device)\n",
        "                x = data_transform(x)\n",
        "                e = torch.randn_like(x[:, 1:, :, :])\n",
        "                b = self.betas\n",
        "\n",
        "                # antithetic sampling\n",
        "                t = torch.randint(low=0, high=self.num_timesteps, size=(n // 2 + 1,)).to(self.device)\n",
        "                t = torch.cat([t, self.num_timesteps - t - 1], dim=0)[:n]\n",
        "                loss = noise_estimation_loss(self.model, x, t, e, b)\n",
        "\n",
        "                if self.step % 10 == 0:\n",
        "                    print(f\"step: {self.step}, loss: {loss.item()}, data time: {data_time / (i+1)}\")\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                self.ema_helper.update(self.model)\n",
        "                data_start = time.time()\n",
        "\n",
        "                if self.step % self.config.training.validation_freq == 0:\n",
        "                    self.model.eval()\n",
        "                    self.sample_validation_patches(val_loader, self.step)\n",
        "\n",
        "                if self.step % self.config.training.snapshot_freq == 0 or self.step == 1:\n",
        "                    save_checkpoint({\n",
        "                        'epoch': epoch + 1,\n",
        "                        'step': self.step,\n",
        "                        'state_dict': self.model.state_dict(),\n",
        "                        'optimizer': self.optimizer.state_dict(),\n",
        "                        'ema_helper': self.ema_helper.state_dict(),\n",
        "                        'params': self.args,\n",
        "                        'config': self.config\n",
        "                    }, filename=os.path.join(self.config.data.data_dir, 'ckpts', self.config.data.dataset + '_ddpm'))\n",
        "\n",
        "    def sample_image(self, x_cond, x, last=True, patch_locs=None, patch_size=None):\n",
        "        skip = self.config.diffusion.num_diffusion_timesteps // self.args.sampling_timesteps\n",
        "        seq = range(0, self.config.diffusion.num_diffusion_timesteps, skip)\n",
        "        if patch_locs is not None:\n",
        "            xs = generalized_steps_overlapping(x, x_cond, seq, self.model, self.betas, eta=0.,\n",
        "                                                              corners=patch_locs, p_size=patch_size)\n",
        "        else:\n",
        "            xs = generalized_steps(x, x_cond, seq, self.model, self.betas, eta=0.)\n",
        "        if last:\n",
        "            xs = xs[0][-1]\n",
        "        return xs\n",
        "\n",
        "    def sample_validation_patches(self, val_loader, step):\n",
        "        image_folder = os.path.join(self.args.image_folder, self.config.data.dataset + str(self.config.data.image_size))\n",
        "        with torch.no_grad():\n",
        "            print(f\"Processing a single batch of validation images at step: {step}\")\n",
        "            for i, (x, y) in enumerate(val_loader):\n",
        "                x = x.flatten(start_dim=0, end_dim=1) if x.ndim == 5 else x\n",
        "                break\n",
        "            n = x.size(0)\n",
        "            x_cond = x[:, :3, :, :].to(self.device)\n",
        "            x_cond = data_transform(x_cond)\n",
        "            x = torch.randn(n, 3, self.config.data.image_size, self.config.data.image_size, device=self.device)\n",
        "            x = self.sample_image(x_cond, x)\n",
        "            x = inverse_data_transform(x)\n",
        "            x_cond = inverse_data_transform(x_cond)\n",
        "\n",
        "            for i in range(n):\n",
        "                save_image(x_cond[i], os.path.join(image_folder, str(step), f\"{i}_cond.png\"))\n",
        "                save_image(x[i], os.path.join(image_folder, str(step), f\"{i}.png\"))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cZjnhJYD2FMH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "# import models\n",
        "import datasets\n",
        "# import utils\n",
        "# import DenoisingDiffusion\n",
        "\n",
        "#  Directly specify these values instead of using argparse\n",
        "CONFIG_PATH = \"config.yml\"         # Your YAML config file\n",
        "RESUME_PATH = \"\"                   # Path to checkpoint (if resuming)\n",
        "SAMPLING_TIMESTEPS = 25            # Validation sampling steps\n",
        "IMAGE_FOLDER = \"results/images/\"   # Where to save validation images\n",
        "SEED = 61                          # Random seed\n",
        "\n",
        "\n",
        "#  Define Args class at the top-level\n",
        "class Args:\n",
        "    def __init__(self, resume, sampling_timesteps, image_folder, seed):\n",
        "        self.resume = resume\n",
        "        self.sampling_timesteps = sampling_timesteps\n",
        "        self.image_folder = image_folder\n",
        "        self.seed = seed\n",
        "\n",
        "\n",
        "def dict2namespace(config):\n",
        "    import argparse\n",
        "    namespace = argparse.Namespace()\n",
        "    for key, value in config.items():\n",
        "        if isinstance(value, dict):\n",
        "            new_value = dict2namespace(value)\n",
        "        else:\n",
        "            new_value = value\n",
        "        setattr(namespace, key, new_value)\n",
        "    return namespace\n",
        "\n",
        "\n",
        "def load_config_and_args():\n",
        "    # Load YAML config\n",
        "    with open(CONFIG_PATH, \"r\") as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    config_ns = dict2namespace(config)\n",
        "\n",
        "    # Return Args instance\n",
        "    args = Args(\n",
        "        resume=RESUME_PATH,\n",
        "        sampling_timesteps=SAMPLING_TIMESTEPS,\n",
        "        image_folder=IMAGE_FOLDER,\n",
        "        seed=SEED\n",
        "    )\n",
        "    return args, config_ns\n",
        "\n",
        "\n",
        "def main():\n",
        "    args, config = load_config_and_args()\n",
        "\n",
        "    # setup device\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "    config.device = device\n",
        "\n",
        "    # set seed\n",
        "    torch.manual_seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # load dataset\n",
        "    print(\"=> using dataset '{}'\".format(config.data.dataset))\n",
        "    DATASET = astro_dataset  # Replace with actual dataset if different\n",
        "\n",
        "    # create model\n",
        "    print(\"=> creating denoising-diffusion model...\")\n",
        "    diffusion = DenoisingDiffusion(args, config)\n",
        "    diffusion.train(DATASET)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Y79f5dvK2gcv",
        "outputId": "7ae1ae47-35fa-43c0-86b1-e19caa125d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "=> using dataset 'Astro'\n",
            "=> creating denoising-diffusion model...\n",
            "=> loading grayscale numpy Astro dataset...\n",
            "epoch:  0\n",
            "step: 10, loss: 913.51953125, data time: 3.8192554712295532\n",
            "step: 20, loss: 354.38800048828125, data time: 1.9110097765922547\n",
            "step: 30, loss: 355.1144714355469, data time: 1.2746771653493245\n",
            "step: 40, loss: 339.60443115234375, data time: 0.9564265608787537\n"
          ]
        }
      ]
    }
  ]
}